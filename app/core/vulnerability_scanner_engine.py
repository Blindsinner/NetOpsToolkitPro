# app/core/vulnerability_scanner_engine.py
import asyncio
import httpx
import re
import random
import time
from typing import AsyncIterator, Dict, Iterable, List, Optional, Set, Tuple
from urllib.parse import urljoin, urlparse, urlencode, parse_qsl, urlunparse

from PySide6.QtCore import QObject, Signal
from bs4 import BeautifulSoup

# --------------------------------------------------------------------------------------
# Advanced, safe-by-default web scanner engine with plugin architecture.
# Emits:
#   vulnerability_found: {"type", "location", "parameter"?, "payload"?, "details"?}
#   progress_updated: str
#   scan_finished: str
#
# Supports scan_types including: "sqli", "xss", "open_redirect", "headers", "csrf", "cookies"
# Your UI currently sends ["sqli","xss"], which are fully supported.
# --------------------------------------------------------------------------------------

# ---------- Utility helpers ----------

def _normalize_url(u: str) -> str:
    """Normalize URL by stripping fragment, normalizing path slashes, and removing trailing slash (except root)."""
    pr = urlparse(u)
    path = pr.path or "/"
    if path != "/" and path.endswith("/"):
        path = path[:-1]
    return urlunparse((pr.scheme, pr.netloc, path, pr.params, pr.query, ""))

def _same_origin(a: str, b: str) -> bool:
    return urlparse(a).netloc == urlparse(b).netloc and urlparse(a).scheme in ("http", "https")

def _safe_join(base: str, href: str) -> str:
    try:
        return _normalize_url(urljoin(base, href))
    except Exception:
        return base

def _uniq(seq: Iterable[str]) -> List[str]:
    seen = set()
    out = []
    for x in seq:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out

def _has_csrf_like_name(name: str) -> bool:
    return bool(re.search(r"csrf|xsrf|token|crumb|authenticity", name or "", re.I))

# ---------- HTTP client factory with retries ----------

class _RetryingClient:
    def __init__(self, *, verify_ssl: bool = False, timeout: float = 10.0, max_retries: int = 2):
        self.client = httpx.AsyncClient(verify=verify_ssl, follow_redirects=True, timeout=timeout)
        self.max_retries = max_retries

    async def close(self):
        await self.client.aclose()

    async def request(self, method: str, url: str, **kwargs) -> httpx.Response:
        delay = 0.5
        for attempt in range(self.max_retries + 1):
            try:
                return await self.client.request(method, url, **kwargs)
            except httpx.RequestError:
                if attempt >= self.max_retries:
                    raise
                await asyncio.sleep(delay)
                delay *= 2

# ---------- Plugin system ----------

class ScanContext:
    def __init__(self, base_url: str, marker: str):
        self.base_url = base_url
        self.marker = marker  # unique reflected marker for XSS-type checks

class BasePlugin:
    """All plugins must implement async test(). They may emit multiple findings."""
    name = "Base"

    async def test(
        self,
        http: _RetryingClient,
        ctx: ScanContext,
        point: Tuple[str, str, Optional[frozenset]],
        emit
    ):
        raise NotImplementedError

# --- SQLi (safe, error/heuristic-based; no time-based) ---

class SQLiPlugin(BasePlugin):
    name = "SQL Injection"
    # Non-destructive tautologies + error-surfacing payloads
    payloads = [
        "' OR '1'='1",
        "\" OR \"1\"=\"1",
        "') OR ('1'='1",
        "1'--",
        "1' ) --",
    ]
    sql_error_markers = [
        "sql syntax", "mysql", "sqlite", "psql:", "postgresql", "unclosed quotation mark",
        "sqlstate", "odbc", "you have an error in your sql"
    ]

    async def test(self, http, ctx, point, emit):
        method, url, params = point
        # Derive candidate parameters
        candidate_params = _extract_param_names(url, params)

        if not candidate_params:
            # fallback to a benign param
            candidate_params = {"q"}

        for key in candidate_params:
            for payload in self.payloads:
                try:
                    req = _build_request(method, url, {key: payload}, fill_others=candidate_params)
                    r = await http.request(**req)
                    if self._looks_like_sqli(r.text):
                        emit({
                            "type": self.name,
                            "location": req["url"],
                            "parameter": key,
                            "payload": payload,
                            "details": "Detected SQL error markers in response."
                        })
                        return
                    # Heuristic: compare with control
                    control = _build_request(method, url, {key: "test"}, fill_others=candidate_params)
                    r2 = await http.request(**control)
                    if _significant_delta(r.text, r2.text):
                        emit({
                            "type": self.name,
                            "location": req["url"],
                            "parameter": key,
                            "payload": payload,
                            "details": "Response changed significantly vs control for SQL-like payload."
                        })
                        return
                except httpx.RequestError:
                    continue

    def _looks_like_sqli(self, text: str) -> bool:
        t = text.lower()
        return any(m in t for m in self.sql_error_markers)

# --- Reflected XSS (marker-based, safe) ---

class XSSPlugin(BasePlugin):
    name = "Reflected XSS"
    # Multiple encodings to try; the 'ctx.marker' is searched in response
    base_payloads = [
        "<{m}>", "\"{m}\"", "'{m}'", "`{m}`", "<img src=x onerror={m}>", "\"><svg/onload={m}>"
    ]
    # The actual JS is not executed; we just reflect a unique marker token
    reflect_marker = "geminitest"

    async def test(self, http, ctx, point, emit):
        method, url, params = point
        candidate_params = _extract_param_names(url, params)
        if not candidate_params:
            candidate_params = {"q"}

        marker = ctx.marker  # unique per scan run
        payloads = [p.format(m=marker) for p in self.base_payloads]

        for key in candidate_params:
            for payload in payloads:
                try:
                    req = _build_request(method, url, {key: payload}, fill_others=candidate_params)
                    r = await http.request(**req)
                    if marker in r.text:
                        emit({
                            "type": self.name,
                            "location": req["url"],
                            "parameter": key,
                            "payload": payload,
                            "details": "Unique marker was reflected unencoded."
                        })
                        return
                except httpx.RequestError:
                    continue

# --- Open Redirect (passive check via Location header) ---

class OpenRedirectPlugin(BasePlugin):
    name = "Open Redirect"
    payload_destinations = [
        "https://example.org",
        "https://evil.example"  # synthetic external domain
    ]

    async def test(self, http, ctx, point, emit):
        method, url, params = point
        candidate_params = _extract_param_names(url, params)
        if not candidate_params:
            candidate_params = {"next", "redirect", "url", "dest"}

        for key in candidate_params:
            for dest in self.payload_destinations:
                try:
                    req = _build_request(method, url, {key: dest}, fill_others=candidate_params)
                    # Disable auto-follow to inspect Location header
                    r = await http.client.request(req["method"], req["url"], data=req.get("data"), follow_redirects=False)
                    loc = r.headers.get("location", "")
                    if loc and (loc.startswith("http://") or loc.startswith("https://")) and not _same_origin(loc, ctx.base_url):
                        emit({
                            "type": self.name,
                            "location": req["url"],
                            "parameter": key,
                            "payload": dest,
                            "details": f"Redirected to external {loc}"
                        })
                        return
                except httpx.RequestError:
                    continue

# --- Security Headers (purely passive, no payloads) ---

class SecurityHeadersPlugin(BasePlugin):
    name = "Security Headers"

    async def test(self, http, ctx, point, emit):
        method, url, _ = point
        if method != "GET":
            return
        try:
            r = await http.request("GET", url)
            missing = []
            # Basic important headers
            expected = [
                "content-security-policy",
                "x-frame-options",
                "x-content-type-options",
                "referrer-policy",
                "strict-transport-security"
            ]
            hdrs = {k.lower(): v for k, v in r.headers.items()}
            for h in expected:
                if h not in hdrs:
                    missing.append(h)
            if missing:
                emit({
                    "type": self.name,
                    "location": url,
                    "details": f"Missing headers: {', '.join(missing)}"
                })
        except httpx.RequestError:
            return

# --- Cookie Flags (passive) ---

class CookieFlagsPlugin(BasePlugin):
    name = "Cookie Flags"

    async def test(self, http, ctx, point, emit):
        method, url, _ = point
        if method != "GET":
            return
        try:
            r = await http.request("GET", url)
            for c in r.headers.get_list("set-cookie"):
                low = c.lower()
                name = c.split("=", 1)[0].strip()
                issues = []
                if "secure" not in low:
                    issues.append("Secure")
                if "httponly" not in low:
                    issues.append("HttpOnly")
                if "samesite" not in low:
                    issues.append("SameSite")
                if issues:
                    emit({
                        "type": self.name,
                        "location": url,
                        "parameter": name,
                        "details": f"Cookie missing flags: {', '.join(issues)}"
                    })
        except httpx.RequestError:
            return

# --- CSRF (heuristic: POST forms lacking csrf-like field) ---

class CSRFPlugin(BasePlugin):
    name = "CSRF Protection"

    async def test(self, http, ctx, point, emit):
        method, url, params = point
        if method != "POST" or params is None:
            return
        names = set(params)
        if not any(_has_csrf_like_name(n) for n in names):
            emit({
                "type": self.name,
                "location": url,
                "details": "POST form appears to lack CSRF token field."
            })

# ---------- Engine ----------

class VulnerabilityScannerEngine(QObject):
    """Advanced engine for crawling and scanning web vulnerabilities."""
    vulnerability_found = Signal(dict)
    progress_updated = Signal(str)
    scan_finished = Signal(str)

    def __init__(self, task_manager):
        super().__init__()
        self.task_manager = task_manager
        self._is_running = False

        # Crawl/scan settings (tuned for safety & responsiveness)
        self._max_pages = 300
        self._max_depth = 3
        self._concurrency = 10
        self._request_timeout = 12.0
        self._respect_robots = True  # polite by default
        self._delay_range = (0.05, 0.2)  # small jitter between requests
        self._semaphore: Optional[asyncio.Semaphore] = None

        # Unique marker for the current scan session (used by XSS plugin)
        self._marker = f"geminitest-{int(time.time())}"

        # Payloads for backward compatibility (not used directly—plugins handle them)
        self.sql_payloads = ["' OR 1=1 --", "' OR '1'='1", "') OR ('1'='1'"]
        self.xss_payload = "<geminitest'\"`>"
        self.xss_check = "geminitest"

        # Available plugins, keyed by scan_types
        self._plugins_map: Dict[str, BasePlugin] = {
            "sqli": SQLiPlugin(),
            "xss": XSSPlugin(),
            "open_redirect": OpenRedirectPlugin(),
            "headers": SecurityHeadersPlugin(),
            "cookies": CookieFlagsPlugin(),
            "csrf": CSRFPlugin(),
        }

    def start_scan(self, base_url: str, scan_types: List[str]):
        if self._is_running:
            return
        self._is_running = True
        # Keep only supported plugin keys
        self._active_plugins: List[BasePlugin] = [
            self._plugins_map[k] for k in scan_types if k in self._plugins_map
        ]
        # Ensure UI defaults still trigger work
        if not self._active_plugins:
            # If UI passed only "sqli"/"xss", they’re already included above; else default to sqli+xss
            self._active_plugins = [self._plugins_map["sqli"], self._plugins_map["xss"]]

        self.task_manager.create_task(self._run_scan(base_url))

    def stop_scan(self):
        self._is_running = False

    async def _run_scan(self, base_url: str):
        base_url = _normalize_url(base_url.strip())
        if not base_url:
            self.scan_finished.emit("Invalid base URL.")
            self._is_running = False
            return
        self.progress_updated.emit(f"Starting scan on {base_url}...")
        self._semaphore = asyncio.Semaphore(self._concurrency)

        points: Set[Tuple[str, str, Optional[frozenset]]] = set()

        try:
            async with _RetryingClient(verify_ssl=False, timeout=self._request_timeout) as http:
                pass
        except TypeError:
            # Older Python won't support 'async with' on this wrapper; use manual close
            http = _RetryingClient(verify_ssl=False, timeout=self._request_timeout)

        try:
            # CRAWL
            async for p in self._crawl(http, base_url):
                if not self._is_running:
                    break
                points.add(p)

            self.progress_updated.emit(f"Crawler found {len(points)} potential injection points.")

            if not self._is_running:
                self.scan_finished.emit("Scan stopped by user during crawl phase.")
                return

            # SCAN — run tests with polite concurrency
            ctx = ScanContext(base_url=base_url, marker=self._marker)
            tasks = []
            for point in points:
                for plugin in self._active_plugins:
                    tasks.append(self._bounded(self._run_plugin(http, ctx, plugin, point)))

            if tasks:
                await asyncio.gather(*tasks, return_exceptions=True)

            self.scan_finished.emit("Vulnerability scan complete.")
        except Exception as e:
            self.scan_finished.emit(f"Scan failed: {e}")
        finally:
            try:
                await http.close()
            except Exception:
                pass
            self._is_running = False

    async def _bounded(self, coro):
        # Concurrency guard + tiny jitter for politeness
        async with self._semaphore:
            await asyncio.sleep(random.uniform(*self._delay_range))
            return await coro

    async def _run_plugin(self, http: _RetryingClient, ctx: ScanContext, plugin: BasePlugin,
                          point: Tuple[str, str, Optional[frozenset]]):
        if not self._is_running:
            return
        try:
            await plugin.test(http, ctx, point, self.vulnerability_found.emit)
        except Exception as e:
            self.progress_updated.emit(f"[{plugin.name}] error: {e}")

    async def _crawl(self, http: _RetryingClient, start_url: str) -> AsyncIterator[Tuple[str, str, Optional[frozenset]]]:
        """Yield tuples of (method, url, params_frozenset_or_None) for GET URLs w/ query and forms."""
        visited: Set[str] = set()
        to_visit: List[Tuple[str, int]] = [(start_url, 0)]
        base_netloc = urlparse(start_url).netloc
        pages_crawled = 0

        robots_disallow: List[str] = []
        if self._respect_robots:
            robots_disallow = await self._fetch_robots(http, start_url)

        while to_visit and self._is_running and pages_crawled < self._max_pages:
            current_url, depth = to_visit.pop(0)
            if depth > self._max_depth:
                continue
            cur_norm = _normalize_url(current_url)
            if cur_norm in visited:
                continue
            visited.add(cur_norm)
            pages_crawled += 1
            self.progress_updated.emit(f"Crawling: {cur_norm}")

            if self._is_blocked_by_robots(cur_norm, robots_disallow, start_url):
                self.progress_updated.emit(f"Skipped by robots.txt: {cur_norm}")
                continue

            try:
                r = await http.request("GET", cur_norm)
                soup = BeautifulSoup(r.text, "html.parser")

                # GET URLs with params become injection points
                pr = urlparse(cur_norm)
                if pr.query:
                    yield ("GET", cur_norm, None)

                # Forms => injection points
                for form in soup.find_all("form"):
                    action = form.get("action") or cur_norm
                    form_url = _safe_join(cur_norm, action)
                    method = form.get("method", "get").upper()
                    inputs = {inp.get("name") for inp in form.find_all("input") if inp.get("name")}
                    yield (method, form_url, frozenset(inputs) if inputs else frozenset())

                # More links to visit
                for a in soup.find_all("a", href=True):
                    href = _safe_join(cur_norm, a["href"])
                    if urlparse(href).netloc != base_netloc:
                        continue
                    if href not in visited:
                        to_visit.append((href, depth + 1))
            except httpx.RequestError:
                continue

    async def _fetch_robots(self, http: _RetryingClient, base_url: str) -> List[str]:
        """Fetch robots.txt and return disallow rules (simple path prefixes)."""
        pr = urlparse(base_url)
        robots_url = f"{pr.scheme}://{pr.netloc}/robots.txt"
        try:
            r = await http.request("GET", robots_url)
            if r.status_code != 200:
                return []
            rules = []
            current_ua = None
            for line in r.text.splitlines():
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                if line.lower().startswith("user-agent:"):
                    current_ua = line.split(":", 1)[1].strip()
                elif current_ua in ("*", None) and line.lower().startswith("disallow:"):
                    path = line.split(":", 1)[1].strip() or "/"
                    rules.append(path)
            return rules
        except httpx.RequestError:
            return []

    def _is_blocked_by_robots(self, url: str, disallow_rules: List[str], base_url: str) -> bool:
        if not disallow_rules:
            return False
        pr = urlparse(url)
        base = urlparse(base_url)
        if pr.netloc != base.netloc:
            return False
        for rule in disallow_rules:
            try:
                if pr.path.startswith(rule):
                    return True
            except Exception:
                continue
        return False


# ---------- Param handling helpers ----------

def _extract_param_names(url: str, params: Optional[frozenset]) -> Set[str]:
    if params:
        return set(params)
    pr = urlparse(url)
    if not pr.query:
        return set()
    names = set()
    for k, _ in parse_qsl(pr.query, keep_blank_values=True):
        if k:
            names.add(k)
    return names

def _build_request(method: str, url: str, override: Dict[str, str],
                   fill_others: Optional[Iterable[str]] = None) -> Dict[str, object]:
    """
    Safely build a GET or POST request dict for httpx with parameters.
    - override: params -> values to set
    - fill_others: for POST, populate other inputs with 'test' to mimic real submission
    """
    method = method.upper()
    pr = urlparse(url)
    base_params = dict(parse_qsl(pr.query, keep_blank_values=True))

    if method == "GET":
        params = {**base_params, **override}
        new_qs = urlencode(params, doseq=True)
        new_url = urlunparse((pr.scheme, pr.netloc, pr.path, pr.params, new_qs, pr.fragment))
        return {"method": "GET", "url": new_url}

    # POST: leave URL as is; send form-encoded body
    body = {}
    if fill_others:
        for k in fill_others:
            body[k] = "test"
    for k, v in override.items():
        body[k] = v
    return {"method": "POST", "url": _normalize_url(url), "data": body}

# ---------- Heuristics ----------

def _significant_delta(a: str, b: str) -> bool:
    """Cheap heuristic: significant length delta or one contains typical DB error words."""
    la, lb = len(a), len(b)
    if lb == 0:
        return False
    # 25% length delta
    if abs(la - lb) / max(lb, 1) >= 0.25:
        return True
    return False

